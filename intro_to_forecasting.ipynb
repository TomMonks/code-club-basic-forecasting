{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Newton Abbot Coding Club\n",
    "# An Introduction to Forecasting using Python\n",
    "\n",
    "### Dull stuff first!\n",
    "*This initial section is a basic primer to help manage time series data structures in Python*\n",
    "\n",
    "Before we can produce forecasts we need to learn how to manipulate and manage dates in Python's NumPy and Pandas libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import dates\n",
    "\n",
    "import seaborn as sns  #seaborn is a nice plotting library that sits on top of matplotlib\n",
    "import matplotlib.style as style\n",
    "style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A quick refresher on NumPy Arrays.\n",
    "\n",
    "NumPy is a high performance library for scientific computing in Python.  It provides the fundermental building block of the SciPy library `numpy.ndarray`\n",
    "\n",
    "You may be familar with Python Lists. Creation and accessing individual array elements and slicing array is very similar to a list. A big difference is that a `numpy.ndarray` requires all data values to be of the **same type**.\n",
    "\n",
    "Suppose that we want a numpy array containing the integers 4, 3, 1, 5 and 6.\n",
    "(Note: a 1D array is referred to as a vector)\n",
    "\n",
    "A simple way to create such an array and access its data is to use the following syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array([4, 3, 1, 5, 6])\n",
    "\n",
    "print(f'the array contains {arr}')\n",
    "print(f'a numpy array has has type {type(arr)}')\n",
    "print(f'the array has a shape of {arr.shape}')\n",
    "print(f'the array has a length of {len(arr)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NumPy arrays are zeroed indexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The item at index 0 in the array is {arr[0]}')\n",
    "print(f'The item at index 2 in the array is {arr[2]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NumPy array slicing\n",
    "\n",
    "`[start:end]` where start is inclusive and end is exclusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The first two elements in the array are {arr[:2]}')\n",
    "print(f'If we want the last two items in the array we can  {arr[3:]}')\n",
    "print(f'Or we can can use negative slice notation {arr[-2:]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datetimes in NumPy\n",
    "\n",
    "If not done correctly, dates and times can be painful to use in coding!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy's data type to manage datetimes is called `datetime64`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static arrays of `datetime64`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(['2019-07-11', '2019-07-12', '2019-07-13', '2019-07-14'], dtype='datetime64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the np.array has dtype='datetime64[D]'.  The 'D' standard for the minimum unit of days\n",
    "Consider an alternative where we include hours.  You need to include the letter 'T' (for timestamp) in string pass to the numpy.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(['2019-07-11T06', '2019-07-12T12', '2019-07-13T01', '2019-07-14T17'], \n",
    "         dtype='datetime64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That time the dtype='datetime64[h]' where 'h' stands for hours.  We can go further and try minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(['2019-07-11T00:13', '2019-07-12T00:15', '2019-07-13T00:15', '2019-07-14T00:05'], \n",
    "         dtype='datetime64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now try seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(['2019-07-11T00:13:59', '2019-07-12T00:15:30', '2019-07-13T00:15:20', '2019-07-14T00:05:15'], \n",
    "         dtype='datetime64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and miliseconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(['2019-07-11T00:13:59.100', '2019-07-12T00:15:30.189'], \n",
    "         dtype='datetime64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick creation of date arrays using `np.arange`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`np.arange(start,stop,step)` (where stop is *exclusive*) is commonly used to produce an np.array of integers can be used to produce an array of evenly-spaced integers (particularly good for generating synthetic testing data).  \n",
    "\n",
    "`np.arange` can also be used to generate a range of date time stamps.\n",
    "\n",
    "*Try changing the step argument to a different value*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange('2019-07-01', '2019-07-31', step=7, dtype='datetime64[D]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange('2019-07-01', '2019-07-31', step=7, dtype='datetime64[m]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get all values in between two dates then *omit* the step argument.  The below generates days between 1st and 10th August"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange('2019-07-01', '2019-07-10', dtype='datetime64[D]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date Time Index in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas `datetimeindex` builds on numpy datetime64 data type.  Pandas is definitely the easiest way to work with time series data in Python.  One of the reasons for this is that pandas can detect and handle different formats of date strings in input files.  Always watch out for problems with US -> UK dates and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to create some synthetic data for testing then you can use the `pandas.date_range` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pd.date_range('1/1/2019', periods=7, freq='D')\n",
    "index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A hourly date range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pd.date_range('1/1/2019', periods=7, freq='h')\n",
    "index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A 'monthly start' range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pd.date_range('1/1/2019', periods=7, freq='MS')\n",
    "index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert numpy array to datetime index**\n",
    "\n",
    "For data manipulation and analysis I often find myself moving between NumPy arrays and pandas DataFrames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_dates = np.array(['2019-07-11', '2019-07-12', '2019-07-13'], dtype='datetime64')\n",
    "index = pd.DatetimeIndex(arr_dates)\n",
    "index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the example above the frequency is **None**.  That's annoying and there are some forecasting tools in Python that will insist on having a frequency.  There are two ways to sort this out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pass in the frequency argument\n",
    "arr_dates = np.array(['2019-07-11', '2019-07-12', '2019-07-13'], dtype='datetime64')\n",
    "index = pd.DatetimeIndex(arr_dates, freq='D')\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the frequency post-hoc\n",
    "arr_dates = np.array(['2019-07-11', '2019-07-12', '2019-07-13'], dtype='datetime64')\n",
    "index = pd.DatetimeIndex(arr_dates)\n",
    "index.freq = 'D'\n",
    "index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding the min|max andin a date time index and accessing a TimeStamp**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(index.min().year)\n",
    "print(index.min().month)\n",
    "print(index.min().day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data from a CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First create a synthetic data set and save to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA = 30\n",
    "PERIODS = 365 * 2\n",
    "\n",
    "idx = pd.date_range('1/1/2018', periods=PERIODS, freq='D')\n",
    "\n",
    "# representing a count variable of sales og widgets with mean LAMBDA.\n",
    "sales = np.random.poisson(LAMBDA, size=PERIODS) \n",
    "df = pd.DataFrame(sales, index=idx)\n",
    "df.columns = ['sales']\n",
    "df.index.name = 'date'\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/example_data1.csv') # save to file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now read in the data and let pandas know that the index is a date field using the `parse_dates` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/example_data1.csv', index_col='date', parse_dates=True)\n",
    "#you have to set this manually\n",
    "df.index.freq='D'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df['sales'].plot(figsize=(12,4))\n",
    "ax.set(xlabel='date', ylabel='sales');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Limit to specific time ranges**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#e.g. first 6 months of 2018 - just use slicing\n",
    "df['sales']['2018-01-01':'2018-07-01'].plot(figsize=(12,4));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Easy ways to get access to specific types of dates\n",
    "\n",
    "A simple way to do this is using the resample function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.resample(rule='A').mean() # year end frequency (Should be 30ish!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.resample(rule='M').max().plot.line(figsize=(12,4)) # month end maximum sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Forecasting\n",
    "\n",
    "We are going to work with a famous time series dataset: monthly airline passengers between 1949 and 1960.  Its a cool data set that is useful for learning how to use forecasting tools.  \n",
    "\n",
    "Our workflow will be:\n",
    "\n",
    "* Load the dataset and setup a datetimeindex\n",
    "* Split the dataset into train and test series\n",
    "* Preprocess the data\n",
    "* Perform a seasonal decomposition\n",
    "* Build a simple baseline forecast\n",
    "* Build a seasonal ARIMA model\n",
    "* Measure the point forecast accuracy of the baseline and ARIMA model\n",
    "* Fit prediction intervals to the model.\n",
    "* Conduct time series cross validation and select the best model\n",
    "* Simulate the forecast\n",
    "* Produce the actual forecast.\n",
    "\n",
    "### Loading the dataset\n",
    "\n",
    "The data set is conveniently part of a library called `pmdarima` which bundles a group classes for \n",
    "ARIMA modelling and some test datasets.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pmdarima\n",
    "import pmdarima as pmd\n",
    "pmd.__version__  #should be version 1.5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmdarima.datasets import load_airpassengers, load_lynx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_DATE = '1949-01-01'\n",
    "airline = load_airpassengers(as_series=True)\n",
    "\n",
    "#there's no datetimeindex from the bundled dataset. So let's add one.\n",
    "airline.index= pd.date_range(START_DATE, periods=len(airline), freq='MS')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(airline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airline.index.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Test Split.  A.K.A. DON'T PLOT IT YET \n",
    "\n",
    "We need to hold back some of our data.  This is so we can simulate forecasting conditions and check a models accuracy on unseen data.  We don't want to know what it looks like as that will introduce bias into the forecasting process and mean we overfit our model to the data we hold.\n",
    "\n",
    "**Remember - there is no such thing as real time data from the future!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_train_test_split(data, split_date):\n",
    "    '''\n",
    "    Split time series into training and test data\n",
    "    \n",
    "    Parameters:\n",
    "    -------\n",
    "    data - pd.DataFrame - time series data.  Index expected as datatimeindex\n",
    "    split_date - the date on which to split the time series\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple (len=2) \n",
    "    0. pandas.DataFrame - training dataset\n",
    "    1. pandas.DataFrame - test dataset\n",
    "    '''\n",
    "    train = data.loc[data.index < split_date]\n",
    "    test = data.loc[data.index >= split_date]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hold back the last twelve months\n",
    "train, test = ts_train_test_split(airline, '1960-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quick check that we have held back 12 months\n",
    "len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the training data\n",
    "\n",
    "Now that we have held back a 'test' set we can safely plot at the airline passengers without introducing bias. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = train.plot(figsize=(12,4))\n",
    "ax.set(xlabel='month', ylabel='air passengers (000s)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing variation due to months having a different number of days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_adj = train / train.index.days_in_month\n",
    "test_adj = train / train.index.days_in_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the adjusted and unadjusted series\n",
    "fig, axes = plt.subplots(2, 1, sharex=True, figsize=(12, 8))\n",
    "\n",
    "axes[0].figsize = (12, 10)\n",
    "\n",
    "axes[0].plot(train)\n",
    "axes[0].set(ylabel='passengers (000s)');\n",
    "\n",
    "axes[1].plot(train_adj)\n",
    "axes[1].set(xlabel='month', ylabel='passengers per day (000s)');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seasonal decomposition\n",
    "\n",
    "Before performing a forecast it is worth decomposing the time series into its components of trend, seasonality and noise.\n",
    "\n",
    "* statsmodels has function called `seasonal_decompose()` for this task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_result = seasonal_decompose(train_adj, model='multiplicative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sd_result.trend.plot(figsize=(12,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sd_result.seasonal.plot(figsize=(12,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sd_result.resid.plot(figsize=(12,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple forecasting baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before employing any complex forecasting approach we are going to use a simple model to create a baseline.  Complex models need to be 'more accurate' than the simple baseline otherwise we throw them out!  We will define how we measure accuracy shortly.\n",
    "\n",
    "Given the strong seasonal component that was confirmed by the seasonal decomposition we are going to use a **seasonal naive** forecasting method.  This is part of the 'carry forward previous values' family of *naive* forecasting methods.  In general, if we have data with period $k$ are at time $t$ and we are predicting time $Y_{t+1}$ then we simply carry forward the value from $Y_{t+1-k}$\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from forecast.baseline import Naive1, SNaive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HORIZON = 12\n",
    "PERIOD = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snf = SNaive(period=PERIOD)\n",
    "snf.fit(train_adj)\n",
    "insample_predictions = snf.fittedvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = train_adj.plot(figsize=(12,4))\n",
    "insample_predictions.plot(ax=ax)\n",
    "ax.set(xlabel='month', ylabel='air passengers (000s)');\n",
    "ax.legend(['Train', 'Predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-Sample Diagnostics\n",
    "\n",
    "To have a look at the in-sample diagnostics take alook at the model **residuals** (difference between the actual value and the prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = snf.resid.plot(figsize=(12,4))\n",
    "ax.set(xlabel='month', ylabel='actual minus predictions');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(snf.resid.dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring the size of the in-sample model error\n",
    "\n",
    "There are many ways to quantify the size of the residuals. Each has its own pro's and con's. One problem is that forecast errors can be both positive and negative and that can mask the true size of the deviations. A simple way to remedy this is to use Mean Absolute Deviation or Mean Squared Error.  There's a bit of debate about if you should take the median value or the mean, but here we will just use the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remember that SNAIVE doesn't fit anything to the first PERIOD data points\n",
    "mean_squared_error(y_true=train_adj[PERIOD:], y_pred=insample_predictions[PERIOD:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(y_true=train_adj[PERIOD:], y_pred=insample_predictions[PERIOD:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean absolute error is conceptually easier to understand.  The dimensions of MSE are airpassengers squared!  Which is odd!  One way to remedy this units issue is the **Root Mean Squared Error (RMSE)**\n",
    "\n",
    "RMSE = $\\sqrt{mean(e_t^2)}$ where $e_t$ is the error in predicting $y_t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tools.eval_measures import rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse(x1=train_adj[PERIOD:], x2=insample_predictions[PERIOD:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#another way!\n",
    "np.sqrt(mean_squared_error(y_true=train_adj[PERIOD:], y_pred=insample_predictions[PERIOD:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE and MAE are called 'scale dependent' measures as the units and magnitude are specific to the problem and context.  An alternative approach is to use a scale invariant measure such as the **mean absolute percentage error (MAPE)**\n",
    "\n",
    "The percentage error is given by $p_t = 100e_t/y_t$ where $e_t$ is the error in predicting $y_t$.  Therefore, MAPE = $mean(|p_t|)$. A limitation of MAPE is that it is inflated when the denominator is small relative to the absolute forecast error (such in the case of outliers or extreme unexpected events). It is also penalises negative errors more than positive errors.  A consequence of this property is that MAPE can lead to selecting a model that tends to under forecast.  The two following examples illustrate the issue. $$APE_{1} = \\left| \\frac{y_t - \\hat{y_t}}{y_t} \\right|= \\left| \\frac{150 - 100}{150} \\right| = \\frac{50}{150} = 33.33\\%$$  \n",
    "\n",
    "$$APE_{2} = \\left| \\frac{100 - 150}{100} \\right| = \\frac{50}{100} = 50\\%$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    '''\n",
    "    MAPE\n",
    "\n",
    "    Parameters:\n",
    "    --------\n",
    "    y_true -- np.array actual observations from time series\n",
    "    y_pred -- the predictions to evaluate\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    float, scalar value representing the MAPE (0-100)\n",
    "    '''\n",
    "    #y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_percentage_error(y_true=train_adj[PERIOD:], y_pred=insample_predictions[PERIOD:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A second baseline.  Naive Forecast 1\n",
    "\n",
    "This method is even simpler than Seasonal Naive. It simply carries forward the previous value. \n",
    "\n",
    "**Task**: \n",
    "\n",
    "* Have a go at using the `Naive1()` class.  It follows the same pattern as SNaive interface.  Instantiate a class.  call the `.fit(y_train)` method and then use the `.fittedvalues` and `.resid` properties for diagnostics.  \n",
    "* Calculate the in-sample RMSE and MAPE\n",
    "* What happens to the insample residuals if you fit the raw training data to the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1**: Create a instanace of `Naive1()`,  fit `train_adj` and store the `.fittedvalues` in a variable called `nf1_fitted_values`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here\n",
    "#nf1 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nf1 = Naive1()\n",
    "nf1.fit(train_adj)\n",
    "nf1_fitted_values = nf1.fittedvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2**: Plot the training time series and fitted_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = train_adj.plot(figsize=(12,4))\n",
    "nf1_fitted_values.plot(ax=ax)\n",
    "ax.set(xlabel='month', ylabel='air passengers');\n",
    "ax.legend(['Train', 'Predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3**: Plot the residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = nf1.resid.plot(figsize=(12,4))\n",
    "ax.set(xlabel='month', ylabel='actual minus predictions');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(nf1.resid.dropna());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4**: Calculate insample rmse and mape scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse(x1=train_adj[1:], x2=nf1_fitted_values[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_percentage_error(y_true=train_adj[1:], y_pred=nf1_fitted_values[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which baseline should we use?\n",
    "\n",
    "We saw the NF1 had a lower **in sample** MAPE and RMSE.  This is effectively the 1-step forecast performance of these methods. To be rigourous in choosing the the best baseline forecast we should really look at **out-of-sample** accuracy for multi-step forecasts. For example, the accuracy when forecasting 6 or 12 months ahead. \n",
    "\n",
    "**IMPORTANT** You could use your **test** dataset to choose your best model (and many examples you will see on the web will do that), but ideally you keep that safely locked away and unexplored until you have selected and tuned a model.  Your test data gives you a *single* chance to simulate forecasting the future - so use it wisely!\n",
    "\n",
    "A good strategy is therefore to further divide *train* to create a **validation** set that can be used to tune and select models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_adj, val_adj = ts_train_test_split(train_adj, '1959-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_adj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_adj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create our models and fit our data.\n",
    "PERIOD = 12\n",
    "\n",
    "nf1 = Naive1()\n",
    "snf = SNaive(period=PERIOD)\n",
    "\n",
    "nf1.fit(train_adj)\n",
    "snf.fit(train_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict 12-steps ahead\n",
    "HORIZON = 12\n",
    "nf1_preds = nf1.predict(horizon=HORIZON)\n",
    "snf_preds = snf.predict(horizon=HORIZON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the model predictions\n",
    "fig, axes = plt.subplots(1, 1, sharex=True, figsize=(12, 8))\n",
    "\n",
    "axes.figsize = (12, 10)\n",
    "\n",
    "axes.plot(val_adj)\n",
    "axes.plot(pd.Series(nf1_preds, index=val_adj.index));\n",
    "axes.plot(pd.Series(snf_preds, index=val_adj.index));\n",
    "axes.set(ylabel='passengers per day(000s)');\n",
    "axes.legend(['validation', 'Naive1', 'SNaive']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive1 Point Forecast Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAPE\n",
    "mean_absolute_percentage_error(y_true=val_adj, y_pred=nf1_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RMSE\n",
    "rmse(x1=val_adj, x2=nf1_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SNaive Point Forecast Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAPE\n",
    "mean_absolute_percentage_error(y_true=val_adj, y_pred=snf_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RMSE\n",
    "rmse(x1=val_adj, x2=snf_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Conclusion**  Its a slightly different picture out of sample!  SNaive is the best simple baseline model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Intervals\n",
    "\n",
    "Prediction intervals are useful because they give an upper and lower bound on point forecast.  You can choose coverage level e.g. 90% or 80%.  In practice prediction intervals are often to narrow or far to wide.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intervals = snf.prediction_interval(horizon=HORIZON, levels=[0.8, 0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#intervals is an list\n",
    "type(intervals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#there are 2 items in this list as we have returned the 80% and 90% prediction intervals\n",
    "len(intervals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#each item in the list is a numpy array\n",
    "type(intervals[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#each numpy array has h rows covering the forecasting horizon and 2 columns for the lower\n",
    "#and upper bound\n",
    "intervals[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the model with 90% predictions intervals\n",
    "fig, axes = plt.subplots(1, 1, sharex=True, figsize=(12, 8))\n",
    "\n",
    "axes.figsize = (12, 10)\n",
    "\n",
    "axes.plot(val_adj)\n",
    "axes.plot(pd.Series(snf_preds, index=val_adj.index));\n",
    "axes.set(ylabel='ln(passengers per day(000s)');\n",
    "\n",
    "limits = pd.DataFrame(intervals[1], index=val_adj.index, columns=['lower', 'upper'])\n",
    "axes.fill_between(val_adj.index, limits['lower'], limits['upper'], \n",
    "                  alpha=.1, \n",
    "                  label='90% PI')\n",
    "\n",
    "axes.legend(['validation', 'SNaive', '90% PI']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what prediction interval coverage have we actually achieved?\n",
    "#will look at the 90% prediction intervals\n",
    "\n",
    "for h in range(2, HORIZON+1, 2):\n",
    "    h_validation = val_adj.iloc[:h].to_numpy()\n",
    "    h_upper = limits['upper'][:h]\n",
    "    h_lower = limits['lower'][:h]\n",
    "    covered = ((h_validation > h_lower) & (h_validation < h_upper)).sum()\n",
    "    coverage = covered / h\n",
    "    print(f'h_{h}: {coverage}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* PI Conclusion.  Poor. Hopefully we can improve on this with a more complex model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back transforming the forecast\n",
    "\n",
    "To reverse our pre-processing we need to:\n",
    "\n",
    "* Multiply by the number of days in the month\n",
    "\n",
    "We will demonstrate that with the validation set here, but the process is the same when producing the forecasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snf_preds = pd.Series(snf_preds, index=val_adj.index)\n",
    "snf_preds_total = snf_preds * snf_preds.index.days_in_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the back transformed series\n",
    "ax = train.plot(figsize=(12,4))\n",
    "snf_preds_total.plot(ax=ax)\n",
    "ax.set(xlabel='month', ylabel='passengers per day (000s)');\n",
    "ax.legend(['train', 'back transformed validation prediction']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what does MAPE look like?\n",
    "mean_absolute_percentage_error(y_true=train[-HORIZON:], y_pred=snf_preds_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can we beat the baseline with a more complex model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to **A**uto**R**egressive **I**ntegrated **M**oving **A**verage (ARIMA) Modelling\n",
    "\n",
    "Before we use ARIMA it is necessary to understand the a two concepts: **autocorrelation** and **stationarity**\n",
    "\n",
    "#### Autocorrelation \n",
    "\n",
    "Autocorrelation (sometimes called serial correlation) describes the correlation of air passengers at time $t$ with air passengers at $t-1, t-2, ... , t-n$. In other words autocorrelation is the correlation of an observation with itself at previous time periods.  Like correlation autocorrelation takes a value between -1.0 ans 1.0\n",
    "\n",
    "Let's have a look at the autocorrelation in the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acf(train_adj);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autocorrelation is a problem for traditional Ordinary Least Squares (OLS) regression because OLS assumes that the error process is normal i.i.d (independent and identitically distributed = there is no pattern).  In the air passengers dataset this clearly isn't the case: there is significant autocorrelation up to lag 12.\n",
    "\n",
    "In the presence of autocorrelation, the standard errors of the coefficients from an OLS regression are too small and cannot be trusted. This is a substative problem for *inference* (i.e making drawing the wrong conclusion, but and it is also a significant disadvantage for forecasting as we are ignoring information that could improve point predictions and prediction intervals.  \n",
    "\n",
    "ARIMA models aim to describe the autocorrelations in the data. \n",
    "\n",
    "**AR** - Stands for Autoregressive. An AR(1) model is a regression model that includes a proportion of lag 1 Y value.  An AR(2) model includes a proportion of lag 1 and lag 2.  An AR(p) model includes a proportion of lags 1 to $p$.  An AR model of order p can be written as:\n",
    "\n",
    "$y_t = c + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} ⋯ \\phi_p y_{t-p} + \\epsilon_t$\n",
    "* where $\\epsilon_t$ is white noise (random error)\n",
    "* The equation above looks like a regression with lagged variables of y\n",
    "\n",
    "\n",
    "**MA** - Stands for Moving Average. An MA(1) model is a regression model that includes a proportion of the lag 1 forecast error value.  An MA(q) model includes a proportion of the errors from lag 1 to q.  An MA model of order q can be written as:\n",
    "\n",
    "$y_t = c + \\epsilon_t + \\theta_1 \\epsilon_{t-1} + \\theta_2 \\epsilon_{t-2} ⋯ \\theta_q \\epsilon_{t-q}$\n",
    "\n",
    "\n",
    "ARIMA models can include a both AR and MA terms at the same time.\n",
    "\n",
    "$y_t = c + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} ⋯ \\phi_p y_{t-p} + \\epsilon_t + \\theta_1 \\epsilon_{t-1} + \\theta_2 \\epsilon_{t-2} ⋯ \\theta_q \\epsilon_{t-q}$\n",
    "\n",
    "#### Seasonal lags\n",
    "ARIMA models can also include seasonal AR and MA terms (along with a seasonal period e.g 12 for monthly data). For example, an AR(P) model where P = 1 would include lag 12.  If P = 2 then it would include lag 12 and lag 24."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stationarity\n",
    "\n",
    "A stationary time series is one whose properties do not depend on the time at which the series is observed.  They have not predictable patterns in the long run.  \n",
    "\n",
    "* Series with trends (i.e. increasing or decreasing over time) are **not** stationary\n",
    "* Series with seasonality (i.e. regular and predictable repeating patterns) are **not** stationary.\n",
    "* A stationary is one with a constant variance as well as mean.\n",
    "\n",
    "ARIMA models require data to be stationary.  The **I** in ARIMA stands for Integrated Root.  An model of I(1) has taken the first or seasonal difference of the data.  A model with I(d) has taken the dth difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_adj.diff(periods=1).plot(figsize=(12,4));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Its clear that the data require additional transformation to stabilise the variance if we are going to use them in an ARIMA model.\n",
    "\n",
    "A simple way to do this is the take the natural logorithm of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_adj = np.log(train_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_adj.diff(periods=1).plot(figsize=(12,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARIMA in Python is easy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pmdarima` package is an excellent forecasting library for building ARIMA models.  I recommend it over and above the options available in core `statsmodels` package.  It is easier to use and offers an `auto_arima()` function that iteratively searches for a model that minimises the **Akaike Information Criterion (AIC)**\n",
    "\n",
    "* ${\\displaystyle \\mathrm {AIC} \\,=\\,2k-2\\ln({\\hat {L}})}$\n",
    "\n",
    "where $k$ = number of parameters in the model and $\\hat{L}$ is the maximum value of the likelihood function for the model.  A likelihood function measures the 'goodness' of fit of a model to data given a set of parameters.  \n",
    "\n",
    "This looks very complicated at first, but all we need to remember that the models we are working with are very flexible. This means that we can easily create complex models that overfit.  Recall that overfitting is when a model will predict the training data exceptionally well, but will perform poorly on out of sample data.  The form of AIC means that it rewards models that fit the training data well, but also penalises models with high $k$ (complicated models with lots of parameters).  That means that AIC will prefer simpler models - in turn reducing overfitting.  That's a great formaula for automatically selecting a good ARIMA forecasting model.\n",
    "\n",
    "There's a large amount of theory about how to build an ARIMA model.  But modern applications tend to opt for the auto approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmdarima.arima import auto_arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select a model that minimises AIC\n",
    "arima_model = auto_arima(train_adj, seasonal=True, m=12, suppress_warnings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_model.plot_diagnostics(figsize=(12,9));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Diagnostics conclusion**.  Model assumptions look reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the ARIMA model to predict the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HORIZON=12\n",
    "arima_preds = arima_model.predict(n_periods=HORIZON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict returns a numpy.array\n",
    "type(arima_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_preds = np.exp(arima_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the model predictions\n",
    "fig, axes = plt.subplots(1, 1, sharex=True, figsize=(12, 8))\n",
    "\n",
    "axes.figsize = (12, 10)\n",
    "\n",
    "axes.plot(val_adj)\n",
    "axes.plot(pd.Series(arima_preds, index=val_adj.index));\n",
    "axes.plot(pd.Series(snf_preds, index=val_adj.index));\n",
    "axes.set(ylabel='passengers per day(000s)');\n",
    "axes.legend(['validation', 'ARIMA', 'SNaive']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Preliminary conclusion.  It looks like the ARIMA model is slightly out performing SNaive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point Forecast Error of the ARIMA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RMSE\n",
    "rmse(x1=val_adj, x2=arima_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAPE\n",
    "mean_absolute_percentage_error(y_true=val_adj, y_pred=arima_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remembering the SNaive MAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_percentage_error(y_true=val_adj, y_pred=snf_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Point prediction conclusion**.  Out of sample accuracy is higher in the ARIMA model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call the predict function with the return_conf_int parameter.\n",
    "#lets go for 90% PIs\n",
    "arima_preds = arima_model.predict(n_periods=HORIZON, return_conf_int=True, alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now that we have asked for a prediction interval we return a tuple\n",
    "type(arima_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#back transform\n",
    "arima_int = np.exp(arima_preds[1])\n",
    "arima_preds = np.exp(arima_preds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the model with 90% predictions intervals\n",
    "fig, axes = plt.subplots(1, 1, sharex=True, figsize=(12, 8))\n",
    "\n",
    "axes.figsize = (12, 10)\n",
    "\n",
    "axes.plot(val_adj)\n",
    "axes.plot(pd.Series(arima_preds, index=val_adj.index));\n",
    "axes.set(ylabel='ln(passengers per day(000s)');\n",
    "\n",
    "limits = pd.DataFrame(arima_int, index=val_adj.index, columns=['lower', 'upper'])\n",
    "axes.fill_between(val_adj.index, limits['lower'], limits['upper'], \n",
    "                  alpha=.1, \n",
    "                  label='90% PI')\n",
    "\n",
    "axes.legend(['validation', 'ARIMA', '90% PI']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what prediction interval coverage have we actually achieved?\n",
    "#will look at the 90% prediction intervals\n",
    "\n",
    "for h in range(2, HORIZON+1, 2):\n",
    "    h_validation = val_adj.iloc[:h].to_numpy()\n",
    "    h_upper = limits['upper'][:h]\n",
    "    h_lower = limits['lower'][:h]\n",
    "    covered = ((h_validation > h_lower) & (h_validation < h_upper)).sum()\n",
    "    coverage = covered / h\n",
    "    print(f'h_{h}: {coverage}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** **PI Conclusion**: Better than Naive model, but still not very good.  Often an underestimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time series cross-validation\n",
    "\n",
    "Up till now we have used a single validation period to select our best model.  The weakness of that approach is that it gives you a sample size of 1 (that's better than nothing, but generally poor statistics!).  Time series cross validation is an approach to provide more data points when comparing models. In the classicial time series literature time series cross validation is called a Rolling Forecasting Horizon.\n",
    "\n",
    "The following code and output provide a simplified view of how rolling forecast horizons work in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_series = [2502, 2414, 2800, 2143, 2708, 1900, 2333, 2222, 1234, 3456]\n",
    "\n",
    "test = full_series[-2:]\n",
    "train = full_series[:-2]\n",
    "print('full training set: {0}'.format(train))\n",
    "print('hidden test set: {0}'.format(test))\n",
    "\n",
    "\n",
    "def show_data_is_used_in_cv(train, min_train_size, horizon):\n",
    "    '''\n",
    "    Loop through training data and illustrate\n",
    "    how it would be used in a rolling forecast approach\n",
    "    to CV.  Assumes that user wants to take steps of 1 in each \n",
    "    fold.\n",
    "    '''\n",
    "    print('\\n**data used in cross-validation**')\n",
    "    print('**horizon = {}'.format(horizon))\n",
    "    for i in range(len(train) - min_train_size - horizon + 1):\n",
    "        print('fold:{0}'.format(i+1))\n",
    "        print('training\\t{0}'.format(train[:min_train_size+i]))\n",
    "        print('validation\\t{0}'.format(train[min_train_size+i:min_train_size+i+horizon]))\n",
    "\n",
    "\n",
    "for horizon in range(1, 4):\n",
    "    show_data_is_used_in_cv(train, min_train_size=4, horizon=horizon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pmdarima` package again includes some neat classes we can use with cross-validating our ARIMA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmdarima.arima import ARIMA\n",
    "from pmdarima.model_selection import RollingForecastCV, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#manually create the arima model\n",
    "model = ARIMA(order=(2,0,0), seasonal_order=(0, 1, 1, 12), suppress_warnings = True)\n",
    "#enforce_invertibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = RollingForecastCV(h=HORIZON, step=1) # initially uses 1/3 of the training size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for cv need all of the training data joined together.\n",
    "airline_adj = airline / airline.index.days_in_month\n",
    "airline_adj = np.log(airline_adj) # transform\n",
    "train, test = ts_train_test_split(airline_adj, '1960-01-01')\n",
    "\n",
    "cv_results = cross_val_score(model, train, cv=cv, scoring=mean_absolute_percentage_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how many splits did we get?\n",
    "#you can go back and try different step values in the RollingForecastCV\n",
    "len(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(cv_results).dropna().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We should also use cross-validation to evaluate for the baseline forecast!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#i've written a function to help with the cv of the baseline\n",
    "from forecast.model_selection import time_series_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#it requires the data to be split into training and validation.\n",
    "train, test = ts_train_test_split(airline, '1960-01-01')\n",
    "train_adj = train / train.index.days_in_month\n",
    "test_adj = train / train.index.days_in_month\n",
    "train_adj, val_adj = train_adj.iloc[:len(train_adj)//3], train_adj.iloc[len(train_adj)//3:]\n",
    "\n",
    "cv_scores = time_series_cv(snf, \n",
    "                           error_func=mean_absolute_percentage_error,\n",
    "                           train=train_adj,\n",
    "                           val=val_adj,\n",
    "                           horizons=[12],\n",
    "                           step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = pd.DataFrame(cv_scores)\n",
    "cv_scores.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **CV Conclusion**: ARIMA is more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulating a Forecast with the TEST set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = ts_train_test_split(airline, '1960-01-01')\n",
    "train_adj = train / train.index.days_in_month\n",
    "test_adj = test / test.index.days_in_month\n",
    "train_adj = np.log(train_adj)\n",
    "\n",
    "model = ARIMA(order=(2,0,0), seasonal_order=(0, 1, 1, 12))\n",
    "model.fit(train_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#90% PIs\n",
    "forecast = model.predict(n_periods=HORIZON, return_conf_int=True, alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#back transform\n",
    "point_forecasts = np.exp(forecast[0])\n",
    "intervals = np.exp(forecast[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the model with 90% predictions intervals\n",
    "fig, axes = plt.subplots(1, 1, sharex=True, figsize=(12, 8))\n",
    "\n",
    "axes.figsize = (12, 10)\n",
    "\n",
    "axes.plot(test_adj)\n",
    "axes.plot(pd.Series(point_forecasts, index=test.index));\n",
    "axes.set(ylabel='passengers per day(000s)');\n",
    "\n",
    "limits = pd.DataFrame(intervals, index=test.index, columns=['lower', 'upper'])\n",
    "axes.fill_between(test.index, limits['lower'], limits['upper'], \n",
    "                  alpha=.1, \n",
    "                  label='90% PI')\n",
    "\n",
    "axes.legend(['test', 'ARIMA', '90% PI']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RMSE\n",
    "rmse(x1=test_adj, x2=point_forecasts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAPE\n",
    "mean_absolute_percentage_error(y_true=test_adj, y_pred=point_forecasts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simulated coverage of the forecast.\n",
    "for h in range(2, HORIZON+1, 2):\n",
    "    h_validation = test_adj.iloc[:h].to_numpy()\n",
    "    h_upper = limits['upper'][:h]\n",
    "    h_lower = limits['lower'][:h]\n",
    "    covered = ((h_validation > h_lower) & (h_validation < h_upper)).sum()\n",
    "    coverage = covered / h\n",
    "    print(f'h_{h}: {coverage}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Produce the final forecast!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HORIZON=12\n",
    "airline_adj = np.log(airline / airline.index.days_in_month)\n",
    "\n",
    "model.fit(airline_adj)\n",
    "final_forecast = model.predict(n_periods=HORIZON, return_conf_int=True, alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#back transform\n",
    "point_forecasts = np.exp(final_forecast[0])\n",
    "intervals = np.exp(final_forecast[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an datetime index for the forecast horizon\n",
    "forecast_index = pd.date_range(start='1961-01-01', periods=HORIZON, freq='MS')\n",
    "\n",
    "#plot the model with 90% predictions intervals\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n",
    "\n",
    "axes[0].plot(np.exp(airline_adj))\n",
    "axes[0].plot(pd.Series(point_forecasts, index=forecast_index));\n",
    "\n",
    "axes[0].set(ylabel='passengers per day(000s)');\n",
    "\n",
    "limits = pd.DataFrame(intervals, index=forecast_index, columns=['lower', 'upper'])\n",
    "axes[0].fill_between(forecast_index, limits['lower'], limits['upper'], \n",
    "                  alpha=.3, \n",
    "                  label='90% PI')\n",
    "\n",
    "axes[0].legend(['Train', 'Forecast', '90% PI']);\n",
    "\n",
    "\n",
    "\n",
    "axes[1].plot(pd.Series(point_forecasts, index=forecast_index));\n",
    "axes[1].set(ylabel='passengers per day(000s)');\n",
    "\n",
    "limits = pd.DataFrame(intervals, index=forecast_index, columns=['lower', 'upper'])\n",
    "axes[1].fill_between(forecast_index, limits['lower'], limits['upper'], \n",
    "                  alpha=.1, \n",
    "                  label='90% PI', color='red')\n",
    "\n",
    "axes[1].legend(['Forecast', '90% PI']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limits['y_hat'] = point_forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limits.to_csv('forecast.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thanks!  Hope you enjoyed this introduction.\n",
    "\n",
    "In your own time have a go **yourself** with the Lynx dataset. The Lynx dataset records the number of skins of predators (lynx) that were collected over many years by the Hudson’s Bay Company (1821 - 1934). It’s commonly used for time-series benchmarking (Brockwell and Davis - 1991)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lynx = load_lynx(as_series=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOLDOUT = 10\n",
    "train, test = lynx[:-HOLDOUT], lynx[-HOLDOUT:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.plot(figsize=(12,4));"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
